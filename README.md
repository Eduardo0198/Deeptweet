<img width="800" height="200" alt="image" src="https://github.com/user-attachments/assets/9dd03a0f-7631-46d6-9a0c-06db2bcffabb" />


# DeepTweet â€“ AnÃ¡lisis de Sentimientos con LSTM

ğŸ¦ DeepTweet â€” Sentiment Analysis con LSTM (2009 â†’ 2025)

DeepTweet es un sistema de anÃ¡lisis de sentimiento diseÃ±ado para clasificar tweets como positivos o negativos, utilizando una arquitectura LSTM Bidireccional y un enfoque iterativo basado en fine-tuning con datos de distintas Ã©pocas.

El objetivo central es demostrar cÃ³mo un modelo clÃ¡sico (entrenado con tweets de 2009) puede evolucionar hasta interpretar lenguaje moderno y tweets reales de 2023â€“2025 mediante ciclos progresivos de actualizaciÃ³n.

---

##  Estructura del Proyecto
```bash
DeepTweet/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_ETL_sentiment140.ipynb        # Limpieza y preparaciÃ³n del dataset base (2009)
â”‚   â”œâ”€â”€ 02_Model_LSTM_Base.ipynb         # Entrenamiento del modelo base (v1)
â”‚   â”œâ”€â”€ 03_ETL_modern.ipynb              # ETL de TweetEval + Kaggle (2016â€“2022)
â”‚   â”œâ”€â”€ 04_Model_FineTuning_Modern.ipynb # Fine-tuning con dataset moderno (v2)
â”‚   â”œâ”€â”€ 05_ETL_Realtime_Tweets.ipynb     # RecolecciÃ³n/limpieza de tweets reales (2023â€“2025)
â”‚   â”œâ”€â”€ 06_Model_FineTuning_Final.ipynb  # Entrenamiento final con tweets reales (v3)
â”‚   â”œâ”€â”€ 07_Interface.ipynb               # Interfaz de predicciÃ³n con el modelo final
â”‚ 
DRIVE
https://drive.google.com/drive/folders/1f3kpO7VQNBeD3GcKodmALRMs7T8MIZUd?usp=sharing
|
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ deeptweet_v1_base.h5
â”‚   â”œâ”€â”€ deeptweet_v2_modern.h5
â”‚   â”œâ”€â”€ deeptweet_v3_final.h5
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sentiment140/
â”‚   â”œâ”€â”€ tweeteval/
â”‚   â”œâ”€â”€ twitter_kaggle/
â”‚   â”œâ”€â”€ realtime/
â”‚   â””â”€â”€ tensors_npy/
â”‚
â””â”€â”€ README.md   # (este documento)
```

Este documento explica **quÃ© hace cada notebook**, quÃ© **recibe**, quÃ© **produce**, y cÃ³mo se conectan entre sÃ­.  
Es una guÃ­a completa para entender el flujo del proyecto.
# ğŸ§  Pipeline General del Proyecto

* 01_ETL_sentiment140.ipynb â†’ genera tensores para v1
* 02_Model_LSTM_Base.ipynb â†’ entrena modelo v1
* 03_ETL_modern.ipynb â†’ genera tensores para v2
* 04_Model_FineTuning_Modern.ipynb â†’ entrena modelo v2 y usa el modelo v1
* 05_ETL_Realtime_Tweets.ipynb â†’ genera tensores para v3
* 06_Model_FineTuning_Final.ipynb â†’ entrena modelo v3 y usa el modelo v2
* 07_Interface.ipynb â†’ carga modelos + pruebas

Todos los datasets, tensores y modelos generados por los ETLs  estÃ¡n organizados en:

 * MODELOS (.h5)
 * PROCESSED_DATA (tensores y un .pkl)
 * RAW_DATA (.csv)

ğŸ“ Drive:
https://drive.google.com/drive/folders/1f3kpO7VQNBeD3GcKodmALRMs7T8MIZUd?usp=sharing

---
# ğŸ¯ Cumplimiento de Competencias

### âœ… 1. Uso de un framework de Deep Learning
El proyecto utiliza **TensorFlow/Keras** para construir, entrenar y ajustar los modelos (LSTM Bidireccional, capas Dense, Dropout, optimizador Adam y callbacks).

### âœ… 2. EvaluaciÃ³n y mejora del modelo
Se entrenaron tres versiones del modelo:
- **v1**: baseline con Sentiment140  
- **v2**: fine-tuning con TweetEval + Kaggle  
- **v3**: fine-tuning final con tweets reales  
Cada iteraciÃ³n incluye evaluaciÃ³n, mÃ©tricas y ajustes para mejorar desempeÃ±o.

### âœ… 3. Uso de datos reales
Se emplearon datasets reales y externos:
- Sentiment140 (1.6M tweets)
- TweetEval (HuggingFace)
- Twitter Sentiment (Kaggle)
- Tweets reales 2023â€“2025 vÃ­a API  
Ninguno pertenece a ejemplos de clase.

### âœ… 4. Predicciones mediante interfaz
El notebook **07_Interface.ipynb** permite cargar los modelos, escribir textos y obtener predicciones en tiempo real, cumpliendo con el requisito de generar resultados desde una interfaz interactiva.


---

# ğŸ““ 01 â€” ETL Sentiment140 (Base)

**Archivo:** `01_ETL_sentiment140.ipynb`

### âœ” Â¿QuÃ© hace?
Prepara el dataset Sentiment140 de 2009 y genera los tensores para entrenar el modelo base.

### âœ” Recibe:
- CSV original de 1.6M tweets con ruido  
- etiquetas {0, 4}

### âœ” Procesa:
- Limpieza profunda:
  - menciones
  - URLs
  - hashtags
  - emojis
  - duplicados
- TokenizaciÃ³n **desde cero**
- Padding
- Train / val / test split

### âœ” Produce (guardado en `data_processed/v1_base/`):
- `tokenizer.pkl`
- `X_train.npy`, `X_val.npy`, `X_test.npy`
- `y_train.npy`, `y_val.npy`, `y_test.npy`

---

# ğŸ““ 02 â€” Modelo LSTM Base (v1)

**Archivo:** `02_Model_LSTM_Base.ipynb`

### âœ” Â¿QuÃ© hace?
Entrena la **versiÃ³n 1** del modelo (baseline).

### âœ” Recibe:
- Tensores procesados (v1)
- Tokenizer

### âœ” Produce:
- `deeptweet_v1_base.h5`

### âœ” Usado por:
â†’ `04_Model_FineTuning_Modern.ipynb`

---

# ğŸ““ 03 â€” ETL Modern (TweetEval + Kaggle)

**Archivo:** `03_ETL_modern.ipynb`

### âœ” Â¿QuÃ© hace?
Prepara un dataset contemporÃ¡neo fusionando:

- TweetEval (HuggingFace)
- Twitter Sentiment (Kaggle)

### âœ” Recibe:
- datasets modernos (raw)

### âœ” Procesa:
- limpieza ligera  
- mapeo a 0/1  
- mezcla de datasets  
- tokenizaciÃ³n **usando tokenizer v1**  
- padding  
- split 60/20/20  

### âœ” Produce (en `data_processed/v2_modern/`):
- `X_train.npy`, `X_val.npy`, `X_test.npy`
- `y_train.npy`, `y_val.npy`, `y_test.npy`

---

# ğŸ““ 04 â€” Fine-Tuning Moderno (v2)

**Archivo:** `04_Model_FineTuning_Modern.ipynb`

### âœ” Â¿QuÃ© hace?
Crea la **versiÃ³n 2** del modelo ajustando el v1 con lenguaje moderno.

### âœ” Recibe:
- tensores v2  
- modelo v1  

### âœ” Procesa:
- congelamiento de capas profundas  
- fine-tuning suave  
- learning rate bajo  

### âœ” Produce:
- `deeptweet_v2_modern.h5`

### âœ” Usado por:
â†’ `06_Model_FineTuning_Final.ipynb`

---

# ğŸ““ 05 â€” ETL Tweets Reales (Live API)

**Archivo:** `05_ETL_Realtime_Tweets.ipynb`

### âœ” Â¿QuÃ© hace?
Procesa tweets reales 2023â€“2025 obtenidos vÃ­a API acadÃ©mica.

### âœ” Recibe:
- JSON con tweets reales

### âœ” Procesa:
- limpieza idÃ©ntica a v1  
- conserva estilo moderno  
- tokenizaciÃ³n con tokenizer v1  
- padding  
- split 70/15/15  

### âœ” Produce (en `data_processed/v3_realtweets/`):
- `X_train_live.npy`, `X_val_live.npy`, `X_test_live.npy`
- `y_train_live.npy`, `y_val_live.npy`, `y_test_live.npy`

---

# ğŸ““ 06 â€” Fine-Tuning Final (v3)

**Archivo:** `06_Model_FineTuning_Final.ipynb`

### âœ” Â¿QuÃ© hace?
Crea la **versiÃ³n final del modelo**, la mÃ¡s precisa y adaptada al lenguaje real.

### âœ” Recibe:
- tensores live (v3)
- modelo v2

### âœ” Procesa:
- early stopping  
- fine-tuning con datos reales  
- mezcla de capas congeladas + entrenables  

### âœ” Produce:
- `deeptweet_v3_final.h5`  
â†’ *modelo recomendado*

---

# ğŸ““ 07 â€” Interfaz / Demo de PredicciÃ³n

**Archivo:** `07_Interface.ipynb`

### âœ” Â¿QuÃ© hace?
Notebook interactivo para **probar los modelos** (v1, v2 o v3).

### âœ” Recibe:
- cualquier modelo `.h5`
- tokenizer `.pkl`

### âœ” El usuario puede:
- cargar modelos manualmente  
- escribir tweets  
- obtener predicciones en tiempo real  

### âœ” Entrada:
Texto escrito por el usuario:




